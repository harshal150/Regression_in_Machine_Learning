{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e816cbf8-2dc4-4ae0-924f-36947f95fcee",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e4426e-a2b1-4b6f-be4f-a8f847c1a2ae",
   "metadata": {},
   "source": [
    "Lasso regression is a type of linear regression that adds a penalty term to the cost function, which encourages the model to use only a subset of the available features. The penalty term is based on the L1 norm of the regression coefficients, which results in some coefficients being shrunk towards zero, effectively performing feature selection. This makes Lasso regression useful when dealing with high-dimensional datasets with many features, as it can help to identify the most important features and reduce the risk of overfitting.\n",
    "\n",
    "In contrast, other regression techniques such as Ridge regression and Ordinary Least Squares do not perform feature selection and may result in overfitting when applied to high-dimensional datasets. Ridge regression adds a penalty term based on the L2 norm of the coefficients, which helps to prevent overfitting but does not perform feature selection. Ordinary Least Squares is a simple linear regression method that estimates the coefficients by minimizing the sum of squared errors between the predicted values and the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e546b24-6422-4865-b9bc-4b3013379fa4",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d5039b-4efc-49ac-ad99-2be970a612af",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is that it can identify and select the most important features while setting the coefficients of less important features to zero. This results in a simpler model that is less prone to overfitting, improves interpretability, and reduces the risk of using irrelevant features. Lasso regression is particularly useful for high-dimensional datasets where there are many features, and it can effectively reduce the dimensionality of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff3750-4cda-4eea-9900-b3d5feb922e2",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4f558-468d-4ab9-90ec-db5b012e0bf4",
   "metadata": {},
   "source": [
    "The coefficients of a Lasso Regression model can be interpreted in the same way as those of a linear regression model. They represent the change in the target variable associated with a one-unit change in the corresponding feature, while holding all other features constant. However, in Lasso Regression, some coefficients may be shrunk towards zero, effectively performing feature selection. A coefficient that is exactly zero indicates that the corresponding feature was not included in the model, while non-zero coefficients indicate that the corresponding feature was included and has a non-zero effect on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46944b47-48fb-4388-b68b-8fdb6ae192e9",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea80d8e-345c-44f3-b973-a7b203fa88ab",
   "metadata": {},
   "source": [
    "The main tuning parameter in Lasso Regression is the regularization strength, which controls the amount of shrinkage applied to the regression coefficients. The strength of regularization is typically controlled by the tuning parameter lambda. Increasing lambda will increase the amount of shrinkage and reduce the complexity of the model, resulting in a simpler model that is less prone to overfitting but may have higher bias. Decreasing lambda will decrease the amount of shrinkage and increase the complexity of the model, resulting in a more complex model that may have lower bias but is more prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66022a38-eafd-4011-a071-b9d61fe4a2c6",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4dd4b-cc3e-466d-baa0-c8de58049759",
   "metadata": {},
   "source": [
    "Lasso Regression is a linear regression technique and can only be used for linear regression problems. However, it can be extended to non-linear regression problems by introducing non-linear transformations of the features. This is known as kernelized Lasso Regression or kernel regression, which uses a kernel function to map the original features into a higher-dimensional space where they may become linearly separable. The Lasso penalty is then applied in this higher-dimensional space, allowing for non-linear feature selection. However, kernelized Lasso Regression can be computationally expensive and may require careful selection of the kernel function and its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a45012-4616-4d5c-839e-1ec4f0471f68",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133eb55c-ebfa-4287-9007-c2fd9bcaba50",
   "metadata": {},
   "source": [
    "The main difference between Ridge Regression and Lasso Regression is in the type of penalty applied to the regression coefficients. Ridge Regression adds a penalty term based on the L2 norm of the coefficients, which results in all coefficients being shrunk towards zero, but none being exactly zero. In contrast, Lasso Regression adds a penalty term based on the L1 norm of the coefficients, which results in some coefficients being set to exactly zero, effectively performing feature selection. This makes Lasso Regression useful for high-dimensional datasets with many features, while Ridge Regression is useful for preventing overfitting in general linear regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a57ffc-bc3c-4d71-8fc9-2553a2797dd9",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a6ac9c-d72f-4db5-ae66-9588f6d457b1",
   "metadata": {},
   "source": [
    "Lasso Regression can handle multicollinearity in the input features to some extent, as it performs feature selection and can effectively remove redundant features that are highly correlated with each other. However, Lasso Regression may not be able to completely eliminate multicollinearity, as it can only select one feature among a group of highly correlated features. In such cases, it may be necessary to apply additional techniques such as principal component analysis (PCA) or partial least squares regression (PLSR) to reduce the dimensionality of the data and address multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd58331-7fe5-45ed-a6d3-62dcdf0dac29",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa3582-079e-45e8-89ad-3405908f4293",
   "metadata": {},
   "source": [
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using cross-validation. This involves dividing the dataset into several subsets, using some of them for training the model with different values of lambda, and then evaluating the performance of each model on the remaining subset. The value of lambda that gives the best performance on the validation set can then be selected as the optimal value. This approach is known as k-fold cross-validation and can help to prevent overfitting and select a value of lambda that generalizes well to new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
